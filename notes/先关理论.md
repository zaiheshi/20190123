- 相关理论

  - 全概率公式
    - 如果事件B1、B2、B3…Bn 构成一个完备事件组，即它们两两互不相容，其和为全集；并且P（Bi)大于0，则对任一事件A有![img](https://upload-images.jianshu.io/upload_images/1531909-deb04a4f3e70c373.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/356/format/webp)
    - 对于任意两随机事件A和B有，![img](https://upload-images.jianshu.io/upload_images/1531909-dd887b3770d02550.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/246/format/webp)
  - 先验概率是指根据以往经验和分析得到的概率。
  - 后验概率是指事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小
  - 例子
    - 一口袋里有3只红球、2只白球，采用不放回方式摸取，第一次摸到红球（记作A）的概率为先验概率，已知第二次摸到了红球，求第一次摸到的是红球的概率为后验概率
  - https://blog.csdn.net/bitcarmanlee/article/details/52201858

- 语言模型

  - 给定句子（词语序列）

    ![img](https://pic4.zhimg.com/80/v2-3c7d97fe9988b1043828fa56f6a1d81f_hd.png)

  - 它的概率可以表示为：

    

    ![img](https://pic2.zhimg.com/80/v2-e8e7c61133d1b23e4d869352aae0c455_hd.png)

  - 两个缺陷

    - **參数空间过大**

    - **数据稀疏严重**， **对于非常多词对的组合，在语料库中都没有出现，依据最大似然估计得到的概率将会是0。**(最大似然估计)

  - 马尔科夫假设

    - 为了解决參数空间过大的问题。引入了马尔科夫假设：**随意一个词出现的概率只与它前面出现的有限的一个或者几个词有关**

    - 如果一个词的出现与它周围的词是独立的，那么我们就称之为unigram也就是一元语言模型

      ![img](https://pic3.zhimg.com/80/v2-dfb6d0be8fa42f803d45e27cb02acf5e_hd.png)

    - 如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram

      ![img](https://pic1.zhimg.com/80/v2-f0e63faeed0dbde5219a3e09778e5b0c_hd.png)

    - 假设一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram

      ![img](https://pic3.zhimg.com/80/v2-4c4b2b156e248bc0dea8812b2b5f0002_hd.png)

    - **在实践中用的最多的就是bigram和trigram了，高于四元的用的非常少，由于训练它须要更庞大的语料，并且数据稀疏严重，时间复杂度高，精度却提高的不多**
    - https://zhuanlan.zhihu.com/p/28080127

- 条件语言模型
  - https://zhuanlan.zhihu.com/p/31453283

     